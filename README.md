# MDA - Metric-Driven Attributions for Vision Transformers
The official implementation of "Metric-Driven Attributions for Vision Transformers" for ICLR 2025.

 * Official Paper - [TBR]()

## How to Use MDA
See `example.ipynb` for a full example of the application of the MDA source code compared with SOTA ViT attribution methods.

It shows:
<ul>
  <li>Importing and setting up models and images for prediction.</li>
  <li>Generating attributions using integrated gradients, gradcam, bidirectional attention, transformer attribution, VIT-CX, and transformer input sampling </li>
  <li>Generating an MDA attribution both sparse.</li>
</ul>

## Replicating the Paper Experiments
To replicate the paper experiments, please see the README inside of the `MDA/experiments` folder.

All experimental data reported in the paper was generated by these tests and the commands in the README.
